{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Oriented Programming: Quick-RAIL, A Simplified RAIL Pipeline\n",
    "##  2024 University of Sydney Hunstead Tutorial 4\n",
    "### Bryan Scott, CIERA | Northwestern University\n",
    "\n",
    "This exercise was originally presented as part of LSST-DA Data Science Fellowship Program Session 21: Software Engineering and Databases held at the University of Illinois Urbana-Champaign, Illinois, United States. It was originally produced by Olivia Lynn, LINCC Frameworks Software Engineer and has been modified for the Hunstead Series by Bryan Scott (CIERA|Northwestern).\n",
    "\n",
    "**R**edshift **A**ssessment and **I**nfrastructure **L**ayers (**RAIL**) is a [LINCC frameworks](https://lsstdiscoveryalliance.org/programs/lincc-frameworks/) project to develop comprehensive analysis infrastructure that will be used for validating redshifts of galaxies from photometry in the LSST Dark Energy Science Collaboration (DESC) and more broadly within the Vera C Rubin Observatory Community.\n",
    "\n",
    "As mentioned in Lecture 1, choice of metrics and sensitivity to errors in classification or parameter estimation are significant outstanding problems in data-driven astronomy. This notebook will walk you through how to write code implementing an analysis and validation pipeline.\n",
    "\n",
    "This heavily references RAIL's Degradation Demo notebook, which can be found [rendered on ReadTheDocs](https://rail-hub.readthedocs.io/projects/rail-notebooks/en/latest/rendered/creation_examples/degradation-demo.html) and in [notebook form on GitHub](https://github.com/LSSTDESC/rail/blob/main/examples/creation_examples/degradation-demo.ipynb).\n",
    "\n",
    "We will work in this notebook with redshift data conditioned on photometric measurements in each of the LSST bands. The goal of this notebook is to understand how object oriented programming works and how the core concepts can be used to write pipelines for working with realistic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports:\n",
    "from numbers import Number\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in truth data generated by PZFlow\n",
    "\n",
    "pzflow is a package developed by John Franklin-Crenshaw (University of Washington) and collaborators for the modelling of redshift probability distributions with normalizing flows. By sampling from such a distribution, one can generate conditional probability distributions of redshifts on colors. Since pzflow does not currently support ARM processors, I have done this for you and saved a truth table that you can read in here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('samples_truth.pickle', 'rb') as handle:\n",
    "    samples_truth = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QUAIL Base Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using a few highly simplified versions of RAIL classes, namely:\n",
    "- QuailStage (accompanied by a NothingStage, to demonstrate how we'll inherit for our other degrader stages)\n",
    "- DataStore\n",
    "- Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class QuailStage(ABC):\n",
    "    \"\"\"A class for the QuailStage stage.\"\"\"\n",
    "\n",
    "    def __init__(self, name):\n",
    "        \"\"\"Constructor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "            The (human-readable) name of the stage (this will be used in the Pipeline's __repr__).\n",
    "        \"\"\"\n",
    "        self.name = name\n",
    "        self.data_in = None\n",
    "        self.data_out = None\n",
    "        \n",
    "    # @abstractmethod is called a decorator - these change the behavior of functions without changing the source code\n",
    "    # this one says that the run method will be implemented in subclasses \n",
    "\n",
    "    @abstractmethod \n",
    "    def run(self):\n",
    "        \"\"\"To be implemented in subclasses. This should set the data_out attribute.\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NothingStage(QuailStage):\n",
    "    \"\"\"A stage that does nothing.\"\"\"\n",
    "    def __init__(self, name):\n",
    "        \"\"\"Constructor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "            The name of the stage.\n",
    "        \"\"\"\n",
    "        super().__init__(name)\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the stage.\"\"\"\n",
    "        self.data_out = self.data_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Degrader: ErrorModel\n",
    "\n",
    "We start by creating an incredibly naive error model as a stand-in for the LSSTErrorModel. To do this, copy the NothingStage Example from above and implement a constructor and apply_errors method. The apply_errors method should loop over the column names (u, g, r, i, z, y) and return the error as a uniform fraction of the data_in for each column. Remember that data_in is an attribute of the QuailStage class, which the ErrorModel stage will inherit from. This will generate u_err, g_err, etc. columns for us, but there's no scientific basis for the values it generates.\n",
    "\n",
    "Once you have done that, implement the run method that was left abstract in QuailStage. The doc-string for the run abstract method tells you what this should return. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BadErrorModel(QuailStage):\n",
    "    \"\"\"A stage that applies bad errors.\"\"\"\n",
    "    def __init__(self, name):\n",
    "        \"\"\"Constructor.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        name : str\n",
    "            The name of the stage.\n",
    "        \"\"\"\n",
    "        super().__init__(name)\n",
    "\n",
    "    def apply_errors(self):\n",
    "        \"\"\"Apply the errors to the input data.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        errors : pandas.DataFrame\n",
    "            The input data with errors applied.\n",
    "        \"\"\"\n",
    "        # create a copy of the input data\n",
    "                \n",
    "            \n",
    "        errors = self.data_in.copy()\n",
    "        \n",
    "        # you implement this\n",
    "        \n",
    "        for col in \n",
    "        \n",
    "            # your code goes here  \n",
    "            \n",
    "        # reorder the columns\n",
    "        errors = errors[[\"redshift\", \"u\", \"u_err\", \"g\", \"g_err\", \"r\", \"r_err\", \"i\", \"i_err\", \"z\", \"z_err\", \"y\", \"y_err\"]]\n",
    "        return errors\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run the stage.\"\"\"\n",
    "        self.data_out = self.apply_errors()\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\"Plot the truth data and the errors.\"\"\"\n",
    "        if self.data_out is None:\n",
    "            raise ValueError(\"You must run the stage first.\")\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(5, 4), dpi=100)\n",
    "\n",
    "        for band in \"ugrizy\":\n",
    "            # pull out the magnitudes and errors\n",
    "            mags = self.data_out[band].to_numpy()\n",
    "            errs = self.data_out[band + \"_err\"].to_numpy()\n",
    "\n",
    "            # sort them by magnitude\n",
    "            mags, errs = mags[mags.argsort()], errs[mags.argsort()]\n",
    "\n",
    "            # plot errs vs mags\n",
    "            ax.plot(mags, errs, label=band)\n",
    "\n",
    "        ax.legend()\n",
    "        ax.set(xlabel=\"Magnitude (AB)\", ylabel=\"Error (mags)\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a DataStore Object\n",
    "\n",
    "At each stage of the pipeline, we will want to keep track of the state of the data we are working with. In this example, we will not modify the colors or redshifts directly, but in general, error model stages will modify the data so that estimators can be applied to determine how each source of error impacts scientific results. \n",
    "\n",
    "Write a data store object. \n",
    "\n",
    "This should have a __getattr__ method that will allow you to access parameters of this class as if they were keys (I've filled this in for you) and a __repr__ method that will print out useful information for instantiated objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStore(dict):\n",
    "    def __init__(self):\n",
    "        dict.__init__(self)\n",
    "    \n",
    "    def __getattr__(self, key): # Code copied from RAIL's DataStore \n",
    "        \"\"\"Allow attribute-like parameter access\"\"\"\n",
    "        try:\n",
    "            return self.__getitem__(key)\n",
    "        except KeyError as msg:\n",
    "            # Kludge to get docstrings to work\n",
    "            if key in [\"__objclass__\"]:  # pragma: no cover\n",
    "                return None\n",
    "            raise KeyError from msg\n",
    "    \n",
    "    # you fill this in \n",
    "    \n",
    "    def __repr__(self): \n",
    "        \"\"\"Prints the DataStore keys and shapes in a human-readable format.\"\"\"\n",
    "        s = \"DataStore\\n\"\n",
    "        for key in self.keys():\n",
    "            s += f\" # you fill this in \\n\"\n",
    "        return s\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making the Pipeline\n",
    "\n",
    "Next we will write a pipeline class. This should have a run method that takes in data in a DataStore Object, the classes and names of the stages you wish to run, and applies the ErrorModel stage we have written, and updates the output_data key of the DataStore. It should also have methods that return the stages in the pipeline. \n",
    "\n",
    "To write the run method, loop over the stages defined when instantiating the pipeline object (these are set by the constructor), and at each stage, update the data_in for the current stage to be the current_data variable. Then run the stage and set the name_data (where name is the name of the stage being run) to the data_out returned from that stage. Then update the current_data to be the data_out from this stage before the next iteration of the loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, data_store, stages):\n",
    "        self.data_store = data_store\n",
    "        self.stages = stages\n",
    "\n",
    "    def run(self):\n",
    "        current_data = self.data_store[\"input_data\"]\n",
    "        \n",
    "        # you implement this \n",
    "        \n",
    "        for stage in self.stages:\n",
    "            print(f\"Running stage: {stage.name}\")\n",
    "            \n",
    "            # your code goes here \n",
    "            \n",
    "        self.data_store[\"output_data\"] = current_data\n",
    "\n",
    "    def __repr__(self):\n",
    "        s = \"Pipeline\\n\"\n",
    "        \n",
    "        # you implement this \n",
    "        \n",
    "        for stage in self.stages:\n",
    "            s += # your code goes here \n",
    "        return s\n",
    "    \n",
    "    def get_stage(self, stage_name):\n",
    "        for stage in self.stages:\n",
    "            if stage.name == stage_name:\n",
    "                return stage\n",
    "        print(f\"Stage {stage_name} not found in pipeline.\")\n",
    "        return None\n",
    "\n",
    "    def plot(self):\n",
    "        pass\n",
    "\n",
    "    def save(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS = DataStore()\n",
    "DS[\"input_data\"] = samples_truth\n",
    "\n",
    "# you implement the stages definition\n",
    "\n",
    "stages = [\n",
    "         # what goes in this list? \n",
    "]\n",
    "\n",
    "pipeline = Pipeline(DS, stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our DataStore before running the pipeline\n",
    "\n",
    "DS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our DataStore after running the pipeline\n",
    "\n",
    "DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking at the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poke around and see the changes you've made! Note that overloading the `__getattr__` in our DataStore class lets us access data via `DS.key` syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS.bad_error_model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the plot method of our error model like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.get_stage(\"bad_error_model\").plot()  # Compare this to the plot in the RAIL degrader notebook,\n",
    "                                              # where we see very different errors!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
