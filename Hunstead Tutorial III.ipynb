{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75dba511",
   "metadata": {},
   "source": [
    "# Introduction to Contemporary Machine Learning\n",
    "\n",
    "##  2024 University of Sydney Hunstead Tutorial 3\n",
    "### Bryan Scott, CIERA | Northwestern University\n",
    "\n",
    "Based on tutorials from LSST DA Data Science Fellowship Program Session 19: Machine Learning held at Drexel University in Philadelphia, Pennsylvania, United States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73c6b0f",
   "metadata": {},
   "source": [
    "## Problem 1: Bayes Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07842041",
   "metadata": {},
   "source": [
    "A good starting point for Machine Learning is the Bayes classifier. The basic idea is to assign the most probable label to each data point using Bayes theorem, we take:\n",
    "\n",
    "$$\n",
    "p(y | x_n) \\propto p(y)p(x_i, ..., x_n | y)\n",
    "$$\n",
    "\n",
    "where y is a label for a data point and the $x_n$ are the features of the data that we want to use to classify each data point. A $\\textit{Naive} Bayes$ classifier makes an important simplifying assumptions that gives it the name - it assumes that the conditional probabilities are independent, $p(x_i, ..., x_n | y) = p(x_i|y)... p(x_n | y)$. That is, the probability of observing any individual feature doesn't depend on any of the other features. Our task is to construct this classifier from a set of examples we've observed previously and compare it to new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40084db8",
   "metadata": {},
   "source": [
    "### Part 0: Load and split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c682ccc",
   "metadata": {},
   "source": [
    "### Loading and splitting the data. \n",
    "\n",
    "Read in the data, then start by selecting the id, fluxes, and object truth type in the lsst data file you've been provided. \n",
    "\n",
    "Once you have selected those, randomly split the data into two arrays, one containing 80% of the data, and a second array containing 20% of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94094b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lsst_data = pd.read_csv('table_data.lsst.cloudapi.csv')\n",
    "\n",
    "# solved \n",
    "\n",
    "lsst_data_to_classify = lsst_data[['id', 'flux_g', 'flux_i', 'flux_r', 'flux_u', 'flux_y', 'flux_z', 'truth_type']]\n",
    "random_data = lsst_data_to_classify.sample(n=1000)\n",
    "\n",
    "train_data = \n",
    "test_data = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff799b",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data[random_data['truth_type'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f2de21",
   "metadata": {},
   "source": [
    "### Part 1: Estimate Class Frequency in the training set\n",
    "\n",
    "One of the ingredients in our classifier is p(y), the unconditional class probabilities. \n",
    "\n",
    "We can get this by counting the number of rows belonging to each class in train_data and dividing by the length of the training data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0186b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solved\n",
    "\n",
    "def estimate_class_probabilities(x_train): \n",
    "    \"\"\"\n",
    "    Computes unconditional class probabilities. \n",
    "     \n",
    "    Args:\n",
    "        x_train (array): training data for the classifier\n",
    " \n",
    "    Returns:\n",
    "        ints p1, p2: unconditional probability of an element of the training set belonging to class 1\n",
    "    \"\"\"\n",
    "    \n",
    "    p1 = \n",
    "    p2 = \n",
    "    \n",
    "    return p1, p2\n",
    "\n",
    "p1, p2 = estimate_class_probabilities(test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1aa268",
   "metadata": {},
   "source": [
    "### Part 2:  Feature Likelihoods\n",
    "\n",
    "We are assuming that the relationship between the classes and feature probabilities are related via:\n",
    "\n",
    "$p(x_i, ..., x_n | y) =  p(x_i|y)... p(x_n | y)$\n",
    "\n",
    "however, we still need to make an assumption about the functional form of the $p(x_n | y)$. As a simple case, we will assume p(x_n | y) follows a Gaussian distribution given by:\n",
    "\n",
    "$$\n",
    "p(x_i | y) = \\frac{1}{\\sqrt{2 \\pi \\sigma_y}} \\exp{\\left(-\\frac{(x_i - \\mu)^2}{\\sigma_y^2}\\right)}\n",
    "$$\n",
    "\n",
    "and we will make a maximum likelihood estimate of $\\mu$ and $\\sigma_y$ from the data. This means using empirical estimates $\\bar{x}$ and $\\hat{\\sigma}$ as estimators of the true parameters $\\mu$ and $\\sigma_y$. \n",
    "\n",
    "Write a fitting function that takes the log of the fluxes and returns an estimate of the parameters of the per-feature likelihoods for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b65b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solved\n",
    "\n",
    "def per_feature_likelihood_parameters(x_train, label):\n",
    "    \"\"\"\n",
    "    Computes MAP estimates for the class conditional likelihood. \n",
    "     \n",
    "    Args:\n",
    "        x_train (array or pd series): training data for the classifier\n",
    "        label (int): training labels for the classifier \n",
    " \n",
    "    Returns:\n",
    "        means, stdevs (array): MAP estimates of the Gaussian conditional probability distributions for a specific class\n",
    "    \"\"\"\n",
    "    \n",
    "    means = \n",
    "    stdevs = \n",
    "    \n",
    "    return means, stdevs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbeb61",
   "metadata": {},
   "source": [
    "### Part 3: MAP Estimates of the Class Probabilities\n",
    "\n",
    "Now that we have the unconditional class probabilities and the parameters of the per feature likelihoods in hand, we can put this all together to build the classifier. Use the methods you have already written to write a function that takes in the training data and returns fit parameters. Once you have done that, write a method that takes the fit parameters as an argument and predicts the class of new (and unseen) data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a17f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the classifier\n",
    "\n",
    "# solved \n",
    "\n",
    "def fit(x_train):\n",
    "    \"\"\"\n",
    "    Convenience function to perform fitting on the training data\n",
    "     \n",
    "    Args:\n",
    "        x_train (array or pd series): training data for the classifier\n",
    " \n",
    "    Returns:\n",
    "        p1, p2, class_1_mean, class_2_mean, class_1_std, class_2_std: see documentation for per_feature_likelihood_parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    p1, p2 = \n",
    "    \n",
    "    class_1_mean, class_1_std = \n",
    "    class_2_mean, class_2_std = \n",
    "    \n",
    "    return p1, p2, class_1_mean, class_2_mean, class_1_std, class_2_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcad9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_probability, class_means, class_dev = fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01867cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# solved \n",
    "\n",
    "def predict(x_test, class_probability, class_means, class_dev):\n",
    "    \"\"\"\n",
    "    Predict method\n",
    "     \n",
    "    Args:\n",
    "        x_test (array): data to perform classification on\n",
    "        class_probability (array): unconditional class probabilities\n",
    "        class_means, class_dev (array): MAP estimates produced by the fit method\n",
    " \n",
    "    Returns:\n",
    "        predict_List (list): class membership predictions\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    flux_g, flux_r, flux_i, flux_u, flux_y = np.log(np.array(x_test[['flux_g', 'flux_r', 'flux_i', 'flux_u', 'flux_y']])).T\n",
    "\n",
    "    prob_1 = \n",
    "    prob_2 = \n",
    "    \n",
    "    predict_list = np.zeros(len(prob_1))\n",
    "        \n",
    "    for i in range(0, len(prob_1)):\n",
    "        if :\n",
    "            predict_list[i] = \n",
    "            \n",
    "        if :\n",
    "            predict_list[i] = \n",
    "    \n",
    "    return predict_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece7794e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data[~(test_data == 0).any(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7865e066",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(test_data, class_probability, class_means, class_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8067d9e3",
   "metadata": {},
   "source": [
    "### Part 4: Metrics\n",
    "\n",
    "After creating a classifier, you now want to evaluate it in terms of how often it correctly and incorrectly classifies the objects in your training set. To do this, we'll design a confusion matrix. A confusion matrix is a matrix whose entries are the counts of the predicted vs actual class. For example, the first entry is the count of objects that are predicted to be of class 1 and actually are of class 1 and so on, while the off-diagonal elements would be instances of class 1 that are predicted to be of class 2, and instances of class 2 that are predicted to be of class 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a366c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(df_confusion, title='Confusion matrix', cmap=plt.cm.gray_r):\n",
    "    plt.matshow(df_confusion, cmap=cmap) # imshow\n",
    "    #plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(df_confusion.columns))\n",
    "    plt.xticks(tick_marks, df_confusion.columns, rotation=45)\n",
    "    plt.yticks(tick_marks, df_confusion.index)\n",
    "    #plt.tight_layout()\n",
    "    plt.ylabel(df_confusion.index.name)\n",
    "    plt.xlabel(df_confusion.columns.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a50e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confusion = pd.crosstab(test_data['truth_type'] ,predict(test_data, class_probability, class_means, class_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fde1da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(df_confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4ba70f",
   "metadata": {},
   "source": [
    "## Problem 2) Generic Perceptron\n",
    "\n",
    "Perceptrons are a type of articifical neuron. We will construct a basic neuron in pure python today and use it to classify data in a \"simple\" two class problem. \n",
    "\n",
    "A representation of a perceptron is given by $w_n \\cdot x_n = \\sum_n w_n x_n$. Each perceptron also has an activation threshold, called a bias, b: \n",
    "\n",
    "$$\\mathrm{output} = \\left\\{ \\begin{array}{lcr}\n",
    "0 & \\mathrm{if} \\; w_n \\cdot x_n + b & \\le 0 \\\\\n",
    "1 & \\mathrm{if} \\; w_n \\cdot x_n + b & > 0\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "\n",
    "Now, build a generic perceptron that can take any collection of input signals and weights, as well as a bias, to determine the binary output from the artificial neuron.\n",
    "\n",
    "**Problem 2a**\n",
    "\n",
    "Write a generic function `perceptron` that takes as input arrays called `signals` and `weights` as well as a float called `bias`. The function should return a boolean indicating whether or not the perceptron is \"activated\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd3d7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron(signals, weights, bias): \n",
    "    '''Generic perceptron function\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    signals : array-like\n",
    "        the input signals for the perceptron\n",
    "        \n",
    "    weights : array-like\n",
    "        the weight applied to each input\n",
    "        \n",
    "    bias : float\n",
    "        the value required for activation\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    activated : bool\n",
    "        whether or not the perceptron is activated\n",
    "    '''\n",
    "    \n",
    "    activated = \n",
    "    return activated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1a9159",
   "metadata": {},
   "source": [
    "**Problem 2b** \n",
    "\n",
    "Is the perceptron activated if the signal = [2.3, 5.3, 1.2, 3.4], the weights = [-3, 2, 0.5, -1], and no bias (i.e., bias = 0)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5b5156",
   "metadata": {},
   "source": [
    "**Problem 2c** \n",
    "\n",
    "What if the signal and weights do not change but the bias = -2? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622d6ce6",
   "metadata": {},
   "source": [
    "**Problem 2d**\n",
    "\n",
    "For a perceptron, training means updating the weights to better reflect the training data. \n",
    "\n",
    "Here's the pseudo-code: \n",
    "  1. Apply the perceptron to one of the data points\n",
    "  2. Adjust the weights and the bias if the perceptron makes an incorrect classification\n",
    "  3. Repeat this procedure over all N datapoints\n",
    "  4. Repeat this procedure for M iterations\n",
    "  \n",
    "How do we adjust the weights? For every sample we evaluate the model error (is the classification correct or not). We then adjust the weight to reduce the error for the following prediction. \n",
    "\n",
    "For a perceptron, these updates can be calculated simply as: \n",
    "\n",
    "$$w_\\mathrm{updated} = w_\\mathrm{current} + \\eta\\,\\, (y_\\mathrm{true} - y_\\mathrm{pred})\\,\\, x,$$\n",
    "\n",
    "$$b_\\mathrm{updated} = b_\\mathrm{current} + \\eta\\,\\, (y_\\mathrm{true} - y_\\mathrm{pred})$$\n",
    "\n",
    "where $w_\\mathrm{updated}$ is the new value for the weight, $w_\\mathrm{current}$ is the current value for the weight, $b_\\mathrm{updated}$ is the new value for the bias, $b_\\mathrm{current}$ is the current value for the bias, $\\eta$ is the called the *learning rate*, $x$ is the value of the input signal, and $(y_\\mathrm{true} - y_\\mathrm{pred})$ captures whether or not the classification was correct.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd763301",
   "metadata": {},
   "source": [
    "Write a function `train_perceptron` that accepts as input `X`, `y`, `weights`, `bias`, `epochs`, and `learning rate`. The function should train the perceptron for $M$ epochs. During each epoch, the weights and bias should be updated using the equation given above while looping over every source in the training set. \n",
    "\n",
    "*Hint* – it is useful to track the number of misclassifications that occur during each epoch. \n",
    "\n",
    "*Hint 2* – for this problem we only care about training, but if you eventually wanted to classify data with the perceptron then you would need to extract the weights and bias from the function, or, even better, write the perceptron as a class object that be trained and also classify (similar to scikit-learn). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13452ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_perceptron(X, y, weights, bias=0, epochs=5, learning_rate=0.01): \n",
    "    '''Train a perceptron to classify binary labels via numerical features\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like\n",
    "        Feature array for the data, in the style of scikit-learn\n",
    "    y : array-like, type = bool\n",
    "        Label array for the data\n",
    "    weights : array-like\n",
    "        Weights for the input signals to the perceptron\n",
    "    bias : array-like\n",
    "        Bias value for the perceptron\n",
    "    epochs : int\n",
    "        Number of instances for training the perceptron\n",
    "    learning_rate : float\n",
    "        Relative step size for tuning the weights and bias\n",
    "    '''\n",
    "    w_updated = weights\n",
    "    bias_updated = bias\n",
    "    misclassified = 0\n",
    "    for signal, true in zip(X, y): \n",
    "        activated = \n",
    "        error = \n",
    "        misclassified += \n",
    "    print(f'For initial weights, accuracy = {(len(y)-misclassified)/len(y):.4f}')\n",
    "    for epoch in range(epochs): \n",
    "        misclassified = 0\n",
    "        for signal, true in zip(X, y): \n",
    "            activated = \n",
    "            error = \n",
    "            misclassified += \n",
    "            for i in range(len(weights)):\n",
    "                w_updated[i] = \n",
    "            bias_updated = \n",
    "            \n",
    "        print(f'For epoch {epoch}, accuracy = {(len(y)-misclassified)/len(y):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52e5a4b",
   "metadata": {},
   "source": [
    "To demonstrate how this works we will generate some synthetic two-dimensional data, but the principle can easily be scaled to an arbitrarily large number of dimensions. \n",
    "\n",
    "We use `scikit-learn` to simulate two classes in a 2d plane using the [`make_blobs()`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) function. We will only include 30 samples so the data are easy to visualize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e5899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377716f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=30, centers=2, n_features=2,\n",
    "                  center_box = (0,4), random_state=1885)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "activated = y == 1\n",
    "ax.plot(X[activated,0], X[activated,1], 'o', \n",
    "        ms = 9, mew=2, mfc='None')\n",
    "ax.plot(X[~activated,0], X[~activated,1], '+', \n",
    "        ms=15, mew=2)\n",
    "ax.set_xlabel('X1', fontsize=15)\n",
    "ax.set_ylabel('X2', fontsize=15)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ecc6e9",
   "metadata": {},
   "source": [
    "Train the perceptron. Use weights of [.1, 1], a bias of 0, train for 20 epochs, with a learning rate $\\eta = 0.005$.\n",
    "\n",
    "Adjust the weights, or bias, or number of epochs, or learning rate, or all of them, to see how the changes affect the output of the perceptron. \n",
    "\n",
    "What do you notice as these changes are made?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e9d19d",
   "metadata": {},
   "source": [
    "## Challenge Problem: Representation Learning and Anomaly Detection\n",
    "\n",
    "Discovery through Eingenbasis Modelling of Uninteresting Data (DEMUD) is an intuitive approach to the anomaly detection problem which depends only on fast matrix algebra, and can therefore be done for large datasets without too many computational challenges. I will not provide a solution aside from noting that an efficient implementation is found here: https://github.com/wkiri/DEMUD?tab=readme-ov-file but I would encourage you to attempt to implement this on your own since it illustrates many of the core concepts in anomaly detection and representation learning from a classical machine learning perspective. I intend to add this to a future Data Science Fellowship Program tutorial, so if you attempt this, please provide feedback that can inform that lesson! \n",
    "\n",
    "The basic idea is to take a dataset and *iteratively* build an eigenbasis model of the data using the *Singular Value Decomposition (SVD)*. Given this model, take data that didn't participate in the construction of the model and compute the reconstruction error of the data. Small reconstruction errors represent data that is well explained by the model, while large reconstruction errors have the potential to be anomalous on the model. Now update the model given the data that remains interesting. \n",
    "\n",
    "An algorithm for doing this is given below. This is taken from https://wkiri.com/research/papers/wagstaff-demud-13.pdf, which introduced this approach and later demonstrated it for data from NASA Mars missions and for image analysis,\n",
    "\n",
    "1. Let X be an input data set represented as an nxd matrix (n rows of data, of d dimensions)\n",
    "2. Initialize an empty set of uninteresting data $X_u = \\emptyset$\n",
    "3. Choose a value for k, the number of principle components that will be used to represent the data. \n",
    "4. Compute the Singular Value Decomposition (U, $\\mu$) of X in terms of k principal components\n",
    "\n",
    "Looping over elements of the input dataset $x \\in X$;\n",
    "\n",
    "5. Each data point x can now be reconstructed from this learned representation via  $\\hat{x} = UU^T (x − \\mu) + \\mu$ (this is a reconstruction of the datapoint $x \\in X$ in the representation retaining k-principal components)  \n",
    "6. Compute the error in the reconstruction $|| x - \\hat{x} ||_2$\n",
    "7. Select the datapoint with the largest reconstruction error. Call this x'. Save the reconstruction error as an explanation of the anomaly represented by the data points. \n",
    "8. Remove x' from the set of data used to construct the model. Add x' to $X_u$. \n",
    "\n",
    "The clever part comes next. When $X_u$ has a single object, the model is then trained on this single datapoint. Now at each subsequent step, add the greatest reconstruction error x' relative to the current model to the set of $X_u$, and \n",
    "9. Compute the SVD of ${X_u, x'}$ relative for k-componets. \n",
    "10. Continue this process for each x until exhaustion. \n",
    "\n",
    "This algorithm shares a common feature with many classical anomaly detection/representation learning approaches in that it says that data which is initially picked out relative to some reconstruction score are likely to be anomalous or worthy of follow up. Something similar is seen with isolation forests, for example, which select data that leads to a split in the tree early on as containing more novel information that later splits. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
